'use client'

import { useState, useRef, useEffect, useCallback } from 'react'
import { MessageCircle, X, Send, Bot, User, Sparkles, Zap, Mic, MicOff } from 'lucide-react'

interface Message {
  id: string
  text: string
  sender: 'user' | 'jarvis'
  timestamp: Date
}

declare global {
  interface Window {
    webkitSpeechRecognition: any
    SpeechRecognition: any
    puter: {
      ai: {
        txt2speech: (text: string) => Promise<HTMLAudioElement>
      }
    }
  }
}

export default function JarvisChat() {
  const [isOpen, setIsOpen] = useState(false)
  const [messages, setMessages] = useState<Message[]>([
    {
      id: '1',
      text: '–ü—Ä–∏–≤–µ—Ç! –Ø –î–∂–∞—Ä–≤–∏—Å, –≤–∞—à–∞ AI-–ø–æ–º–æ—â–Ω–∏—Ü–∞. –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?',
      sender: 'jarvis',
      timestamp: new Date()
    }
  ])
  const [inputMessage, setInputMessage] = useState('')
  const [isTyping, setIsTyping] = useState(false)
  const [isRecording, setIsRecording] = useState(false)
  const [isListening, setIsListening] = useState(false)
  const [speechSupported, setSpeechSupported] = useState(false)
  const [ttsSupported, setTtsSupported] = useState(false)
  const [isSpeaking, setIsSpeaking] = useState(false)
  const messagesEndRef = useRef<HTMLDivElement>(null)
  const inputRef = useRef<HTMLInputElement>(null)
  const recognitionRef = useRef<any>(null)
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const isRecordingRef = useRef(false)
  const currentTranscriptRef = useRef('')
  const speechSynthesisRef = useRef<SpeechSynthesis | null>(null)
  const fullTextRef = useRef<string>('')
  const currentAudioRef = useRef<HTMLAudioElement | null>(null)
  const isStreamingRef = useRef<boolean>(false)

  const scrollToBottom = () => {
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º requestAnimationFrame –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–π –ø—Ä–æ–∫—Ä—É—Ç–∫–∏
    requestAnimationFrame(() => {
      if (messagesEndRef.current) {
        messagesEndRef.current.scrollIntoView({
          behavior: 'smooth',
          block: 'end',
          inline: 'nearest'
        })
      }

      // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–∫—Ä—É—Ç–∫–∞ –¥–ª—è —Å–ª—É—á–∞–µ–≤ –∫–æ–≥–¥–∞ scrollIntoView –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
      const chatMessages = document.querySelector('.chat-messages')
      if (chatMessages) {
        chatMessages.scrollTop = chatMessages.scrollHeight
      }
    })
  }

  useEffect(() => {
    scrollToBottom()
  }, [messages])

  useEffect(() => {
    if (isOpen && inputRef.current) {
      inputRef.current.focus()

      // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–∑–≤—É—á–∏–≤–∞–µ–º –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –ø—Ä–∏ –æ—Ç–∫—Ä—ã—Ç–∏–∏ —á–∞—Ç–∞
      if (messages.length === 1) {
        // –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã —á–∞—Ç —É—Å–ø–µ–ª –æ—Ç–∫—Ä—ã—Ç—å—Å—è
        setTimeout(() => {
          console.log('Auto-playing greeting...')
          speakText(messages[0].text)
        }, 500)
      }
    }
  }, [isOpen, messages])

  // –û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ñ–æ–∫—É—Å–∞ –Ω–∞ –ø–æ–ª–µ –≤–≤–æ–¥–∞ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤
  const handleInputFocus = () => {
    // –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é –ø—Ä–∏ —Ñ–æ–∫—É—Å–µ (–¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö)
    setTimeout(() => {
      scrollToBottom()
    }, 300) // –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –ø–æ—è–≤–ª–µ–Ω–∏—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã
  }

  // –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –ø–æ—Ç–µ—Ä–∏ —Ñ–æ–∫—É—Å–∞
  const handleInputBlur = () => {
    // –ü—Ä–æ–∫—Ä—É—Ç–∫–∞ –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é –ø—Ä–∏ –ø–æ—Ç–µ—Ä–µ —Ñ–æ–∫—É—Å–∞
    setTimeout(() => {
      scrollToBottom()
    }, 100)
  }

  // –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ –¥–ª—è –∞–≤—Ç–æ–ø—Ä–æ–∫—Ä—É—Ç–∫–∏
  useEffect(() => {
    if (!isOpen) return

    const resizeObserver = new ResizeObserver(() => {
      // –ü—Ä–æ–∫—Ä—É—á–∏–≤–∞–µ–º –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ä–∞–∑–º–µ—Ä–∞
      setTimeout(() => {
        scrollToBottom()
      }, 50)
    })

    const chatContainer = document.querySelector('.chat-messages')
    if (chatContainer) {
      resizeObserver.observe(chatContainer)
    }

    // –¢–∞–∫–∂–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è viewport –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö
    const handleResize = () => {
      setTimeout(() => {
        scrollToBottom()
      }, 300)
    }

    window.addEventListener('resize', handleResize)
    // –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º —Å–æ–±—ã—Ç–∏—è –ø–æ–∫–∞–∑–∞/—Å–∫—Ä—ã—Ç–∏—è –∫–ª–∞–≤–∏–∞—Ç—É—Ä—ã –Ω–∞ –º–æ–±–∏–ª—å–Ω—ã—Ö
    window.addEventListener('orientationchange', handleResize)

    return () => {
      resizeObserver.disconnect()
      window.removeEventListener('resize', handleResize)
      window.removeEventListener('orientationchange', handleResize)
    }
  }, [isOpen])

  // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Speech Recognition
  useEffect(() => {
    console.log('Initializing Speech Recognition...')
    if (typeof window !== 'undefined' && ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window)) {
      console.log('Speech Recognition API is supported')
      setSpeechSupported(true)
      const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition
      const recognition = new SpeechRecognition()
      
      recognition.continuous = true   // –í–∫–ª—é—á–∞–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ
      recognition.interimResults = true
      recognition.lang = 'ru-RU'
      recognition.maxAlternatives = 1
      
      recognition.onstart = () => {
        console.log('Speech recognition started')
        setIsListening(true)
      }
      
      recognition.onresult = (event: any) => {
        let finalTranscript = ''
        let interimTranscript = ''
        
        for (let i = event.resultIndex; i < event.results.length; i++) {
          const transcript = event.results[i][0].transcript
          if (event.results[i].isFinal) {
            finalTranscript += transcript
          } else {
            interimTranscript += transcript
          }
        }
        
        if (finalTranscript) {
          const trimmedTranscript = finalTranscript.trim()
          currentTranscriptRef.current = trimmedTranscript
          setInputMessage(trimmedTranscript)
          console.log('Final transcript received:', trimmedTranscript)
          
          // –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä –Ω–∞ —Å–µ–∫—É–Ω–¥—É –º–æ–ª—á–∞–Ω–∏—è
          if (silenceTimerRef.current) {
            clearTimeout(silenceTimerRef.current)
          }
          
          silenceTimerRef.current = setTimeout(() => {
            console.log('Silence timer fired, transcript:', trimmedTranscript)
            // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å, –µ—Å–ª–∏ –µ—Å—Ç—å —Ç–µ–∫—Å—Ç
            if (trimmedTranscript && isRecordingRef.current) {
              console.log('Sending message:', trimmedTranscript)
              stopRecording()
              sendMessage(trimmedTranscript)
            }
          }, 3000)
        } else if (interimTranscript) {
          currentTranscriptRef.current = interimTranscript.trim()
          setInputMessage(interimTranscript.trim())
        }
      }
      
      recognition.onerror = (event: any) => {
        console.error('Speech recognition error:', event.error)
        setIsRecording(false)
        isRecordingRef.current = false
        setIsListening(false)
        
        // –û—á–∏—â–∞–µ–º —Ç–∞–π–º–µ—Ä –ø—Ä–∏ –æ—à–∏–±–∫–µ
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current)
        }
        
        // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –æ—à–∏–±–∫–∏
        switch (event.error) {
          case 'aborted':
            console.log('Speech recognition was aborted')
            break
          case 'not-allowed':
            console.log('Microphone access denied')
            alert('–ù—É–∂–Ω–æ —Ä–∞–∑—Ä–µ—à–∏—ÇÔøΩÔøΩ –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞')
            break
          case 'no-speech':
            console.log('No speech detected - continuing to listen')
            // –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å–ª—É—à–∞—Ç—å –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ —Ä–µ—á–∏
            if (isRecordingRef.current) {
              setTimeout(() => {
                if (isRecordingRef.current && recognitionRef.current) {
                  try {
                    recognitionRef.current.start()
                  } catch (restartError) {
                    console.log('Failed to restart after no-speech:', restartError)
                  }
                }
              }, 500)
            }
            return // –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å
          case 'network':
            console.log('Network error - retrying...')
            if (isRecordingRef.current) {
              setTimeout(() => {
                if (isRecordingRef.current && recognitionRef.current) {
                  try {
                    recognitionRef.current.start()
                  } catch (restartError) {
                    console.log('Failed to restart after network error:', restartError)
                  }
                }
              }, 1000)
            }
            return
          default:
            console.log('Speech recognition error:', event.error)
        }
      }
      
      recognition.onend = () => {
        console.log('Speech recognition ended')
        setIsListening(false)
        // –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ, –µ—Å–ª–∏ –µ—â–µ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º
        if (isRecordingRef.current) {
          setTimeout(() => {
            if (isRecordingRef.current) {
              try {
                console.log('Auto-restarting speech recognition...')
                recognition.start()
              } catch (error) {
                console.log('Failed to restart recognition:', error)
                // ÔøΩÔøΩ—Ä–∏ –æ—à–∏–±–∫–µ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å—å
                setIsRecording(false)
                isRecordingRef.current = false
                setIsListening(false)
              }
            }
          }, 300) // –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º
        }
      }
      
      recognitionRef.current = recognition
    } else {
      setSpeechSupported(false)
      console.log('Speech Recognition not supported in this browser')
    }

    // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è TTS - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ ru-RU-SvetlanaNeural (–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–æ–ª–æ—Å–∞ –∏–¥–µ–∞–ª—å–Ω—ã–µ)
    const initTTS = () => {
      if (typeof window !== 'undefined') {
        setTtsSupported(true)
        console.log('TTS initialized with ru-RU-SvetlanaNeural (–Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–æ–ª–æ—Å–∞ –∏–¥–µ–∞–ª—å–Ω—ã–µ)')
        
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Web Speech API —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏ stopSpeaking
        if ('speechSynthesis' in window) {
          speechSynthesisRef.current = window.speechSynthesis
        }
      } else {
        setTtsSupported(false)
        console.log('Browser TTS not supported')
      }
    }

    initTTS()

    return () => {
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current)
      }
      if (recognitionRef.current && isRecordingRef.current) {
        isRecordingRef.current = false
        setIsRecording(false)
        setIsListening(false)
        try {
          recognitionRef.current.stop()
        } catch (error) {
          console.log('Recognition cleanup error:', error)
        }
      }
      // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–µ—á—å –ø—Ä–∏ —Ä–∞–∑–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏
      if (speechSynthesisRef.current && speechSynthesisRef.current.speaking) {
        speechSynthesisRef.current.cancel()
      }
    }
  }, [])

  const startRecording = async () => {
    console.log('startRecording called, current state:', { isRecording, isListening })
    if (recognitionRef.current && !isRecording && !isListening) {
      try {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
        if (navigator.mediaDevices && navigator.mediaDevices.getUserMedia) {
          try {
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
            stream.getTracks().forEach(track => track.stop()) // –°—Ä–∞–∑—É –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º stream
          } catch (permissionError) {
            console.error('Microphone permission denied:', permissionError)
            return
          }
        }
        
        console.log('Starting speech recognition...')
        setIsRecording(true)
        isRecordingRef.current = true
        currentTranscriptRef.current = ''
        setInputMessage('')
        recognitionRef.current.start()
      } catch (error) {
        console.error('Failed to start speech recognition:', error)
        setIsRecording(false)
        isRecordingRef.current = false
        setIsListening(false)
      }
    } else {
      console.log('Cannot start recording - conditions not met')
    }
  }

  const stopRecording = () => {
    console.log('stopRecording called')
    if (recognitionRef.current) {
      // –°–Ω–∞—á–∞–ª–∞ –æ–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
      setIsRecording(false)
      isRecordingRef.current = false
      setIsListening(false)
      
      // –û—á–∏—â–∞–µ–º —Ç–∞–π–º–µ—Ä
      if (silenceTimerRef.current) {
        clearTimeout(silenceTimerRef.current)
        silenceTimerRef.current = null
      }
      
      // –ó–∞—Ç–µ–º –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º recognition
      try {
        if (recognitionRef.current) {
          console.log('Stopping speech recognition...')
          recognitionRef.current.stop()
        }
      } catch (error) {
        console.log('Recognition stop error:', error)
      }
    }
  }

  const toggleRecording = async () => {
    if (isRecording) {
      stopRecording()
    } else {
      await startRecording()
    }
  }

  // –û–∑–≤—É—á–∏–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ–ª–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
  const speakCompleteText = async (text: string) => {
    if (!text.trim()) return

    console.log('üé§ –û–∑–≤—É—á–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç:', text.length, '—Å–∏–º–≤–æ–ª–æ–≤')

    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ –∞—É–¥–∏–æ
    if (currentAudioRef.current) {
      currentAudioRef.current.pause()
      currentAudioRef.current = null
    }

    setIsSpeaking(true)

    try {
      const response = await fetch('/api/tts', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Accept': 'audio/mpeg'
        },
        body: JSON.stringify({
          text: text,
          rate: '0.95'
        })
      })

      if (!response.ok) {
        throw new Error(`TTS API error: ${response.statusText}`)
      }

      const audioBlob = await response.blob()
      const audioUrl = URL.createObjectURL(audioBlob)
      const audio = new Audio(audioUrl)

      currentAudioRef.current = audio

      audio.onended = () => {
        URL.revokeObjectURL(audioUrl)
        setIsSpeaking(false)
        currentAudioRef.current = null
        console.log('üéµ –û–∑–≤—É—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ')
      }

      audio.onerror = () => {
        URL.revokeObjectURL(audioUrl)
        setIsSpeaking(false)
        currentAudioRef.current = null
      }

      await audio.play()

    } catch (error) {
      console.error('Speech error:', error)
      setIsSpeaking(false)
    }
  }

  const startNewSpeech = () => {
    // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –±—É—Ñ–µ—Ä –¥–ª—è –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
    fullTextRef.current = ''
    isStreamingRef.current = true

    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ –∞—É–¥–∏–æ
    if (currentAudioRef.current) {
      currentAudioRef.current.pause()
      currentAudioRef.current = null
    }

    setIsSpeaking(false)
    console.log('üé§ –ì–æ—Ç–æ–≤–∏–º—Å—è –∫ –æ–∑–≤—É—á–∏–≤–∞–Ω–∏—é –Ω–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞')
  }

  const speakText = async (text: string) => {
    // –ü—Ä–æ—Å—Ç–æ –æ–∑–≤—É—á–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
    console.log('üéµ –û–∑–≤—É—á–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç:', text)
    await speakCompleteText(text)
  }

  const stopSpeaking = () => {
    // –û—á–∏—â–∞–µ–º –±—É—Ñ–µ—Ä—ã
    fullTextRef.current = ''
    isStreamingRef.current = false

    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–µ–∫—É—â–µ–µ –∞—É–¥–∏–æ
    if (currentAudioRef.current) {
      currentAudioRef.current.pause()
      currentAudioRef.current = null
    }

    setIsSpeaking(false)

    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Web Speech API
    if (speechSynthesisRef.current && speechSynthesisRef.current.speaking) {
      speechSynthesisRef.current.cancel()
    }

    // –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ HTML Audio —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
    const audioElements = document.querySelectorAll('audio')
    audioElements.forEach(audio => {
      if (!audio.paused) {
        audio.pause()
        audio.currentTime = 0
      }
    })

    console.log('üõë –†–µ—á—å –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞')
  }

  const sendMessage = async (message: string) => {
    console.log('sendMessage called with:', message)
    if (!message.trim()) {
      console.log('Message is empty, not sending')
      return
    }

    const userMessage: Message = {
      id: Date.now().toString(),
      text: message,
      sender: 'user',
      timestamp: new Date()
    }

    console.log('Sending message to AI:', userMessage.text)
    setMessages(prev => [...prev, userMessage])
    setInputMessage('')
    setIsTyping(true)

    try {
      // –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è AI
      const allMessages = [...messages, userMessage]
      const aiMessages = allMessages.map(msg => ({
        role: msg.sender === 'user' ? 'user' as const : 'assistant' as const,
        content: msg.text
      }))

      // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ—Ç–æ–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∫ AI API
      const response = await fetch('/api/chat', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          messages: aiMessages,
          stream: true // –í–∫–ª—é—á–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤—É—é –ø–µ—Ä–µ–¥–∞—á—É
        })
      })

      if (!response.ok) {
        throw new Error(`AI API error: ${response.status} ${response.statusText}`)
      }

      // –°–æ–∑–¥–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –î–∂–∞—Ä–≤–∏—Å–∞ –¥–ª—è –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
      const jarvisMessageId = (Date.now() + 1).toString()
      const jarvisMessage: Message = {
        id: jarvisMessageId,
        text: '',
        sender: 'jarvis',
        timestamp: new Date()
      }

      // –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ–º –æ–±–Ω–æ–≤–ª—è—Ç—å
      setMessages(prev => [...prev, jarvisMessage])
      setIsTyping(false)

      // –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤–æ–µ –æ–∑–≤—É—á–∏–≤–∞–Ω–∏–µ
      startNewSpeech()

      // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç
      const reader = response.body?.getReader()
      const decoder = new TextDecoder()
      let accumulatedText = ''
      let sentenceBuffer = ''

      if (reader) {
        while (true) {
          const { done, value } = await reader.read()
          
          if (done) break

          const chunk = decoder.decode(value, { stream: true })
          const lines = chunk.split('\n')

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const data = line.slice(6)
              
              if (data === '[DONE]') continue

              try {
                const parsed = JSON.parse(data)
                const content = parsed.choices?.[0]?.delta?.content || ''
                
                if (content) {
                  accumulatedText += content
                  sentenceBuffer += content

                  // –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
                  setMessages(prev => prev.map(msg => 
                    msg.id === jarvisMessageId 
                      ? { ...msg, text: accumulatedText }
                      : msg
                  ))

                  // –ü—Ä–æ—Å—Ç–æ –Ω–∞–∫–∞–ø–ª–∏–≤–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–ª—è –æ–∑–≤—É—á–∏–≤–∞–Ω–∏—è –≤ –∫–æ–Ω—Ü–µ
                  fullTextRef.current += content
                }
              } catch (e) {
                console.log('Parse error:', e)
              }
            }
          }
        }
      }

      // –ó–∞–≤–µ—Ä—à–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –∏ –æ–∑–≤—É—á–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç
      isStreamingRef.current = false

      // –ñ–¥–µ–º –Ω–µ–±–æ–ª—å—à—É—é –ø–∞—É–∑—É –∏ –æ–∑–≤—É—á–∏–≤–∞–µ–º –≤–µ—Å—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Ü–µ–ª–∏–∫–æ–º
      setTimeout(() => {
        const fullText = fullTextRef.current.trim()
        if (fullText) {
          console.log('üé§ –û–∑–≤—É—á–∏–≤–∞–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç:', fullText.length, '—Å–∏–º–≤–æ–ª–æ–≤')
          speakCompleteText(fullText)
        }
      }, 300)

    } catch (error) {
      console.error('AI chat error:', error)
      
      // Fallback to predefined responses if AI fails
      const fallbackResponses = [
        '–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑ —á–µ—Ä–µ–∑ –ø–∞—Ä—É —Å–µ–∫—É–Ω–¥.',
        '–ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π—Ç–µ –≤–æ–ø—Ä–æ—Å, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞.',
        '–í—Ä–µ–º–µ–Ω–Ω—ã–π —Å–±–æ–π. –î–∞–≤–∞–π—Ç–µ –ø–æ–ø—Ä–æ–±—É–µ–º —Å–Ω–æ–≤–∞.'
      ]
      
      const fallbackResponse = fallbackResponses[Math.floor(Math.random() * fallbackResponses.length)]
      
      const jarvisMessage: Message = {
        id: (Date.now() + 1).toString(),
        text: fallbackResponse,
        sender: 'jarvis',
        timestamp: new Date()
      }

      setMessages(prev => [...prev, jarvisMessage])
      setIsTyping(false)

      // –û–∑–≤—É—á–∏–≤–∞–µ–º fallback –æ—Ç–≤–µ—Ç
      setTimeout(async () => {
        await speakText(fallbackResponse)
      }, 500)
    }
  }

  const handleSendMessage = async () => {
    await sendMessage(inputMessage)
  }

  const handleKeyPress = (e: React.KeyboardEvent) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault()
      handleSendMessage()
    }
  }

  const toggleChat = () => {
    if (isOpen) {
      if (isRecording) {
        stopRecording()
      }
      if (isSpeaking) {
        stopSpeaking()
      }
    }
    setIsOpen(!isOpen)
  }

  return (
    <>
      {/* –ö–Ω–æ–ø–∫–∞ —á–∞—Ç–∞ –≤ –ø—Ä–∞–≤–æ–º –Ω–∏–∂–Ω–µ–º —É–≥–ª—É */}
      {!isOpen && (
        <div className="chat-button-container">
          <button
            onClick={toggleChat}
            className="chat-button"
            aria-label="–û—Ç–∫—Ä—ã—Ç—å —á–∞—Ç —Å –î–∂–∞—Ä–≤–∏—Å–æ–º"
          >
            <MessageCircle className="chat-button-icon" />
            <div className="chat-button-pulse"></div>
          </button>
          <div className="chat-button-tooltip">
            –ß–∞—Ç —Å –î–∂–∞—Ä–≤–∏—Å–æ–º
          </div>
        </div>
      )}

      {/* –ü–æ–ª–Ω–æ—ç–∫—Ä–∞–Ω–Ω—ã–π —á–∞—Ç */}
      {isOpen && (
        <div className="chat-overlay">
          <div className="chat-container">
            {/* –ó–∞–≥–æ–ª–æ–≤–æ–∫ —á–∞—Ç–∞ */}
            <div className="chat-header">
              <div className="chat-header-info">
                <div className="chat-avatar">
                  <Bot className="chat-avatar-icon" />
                </div>
                <div className="chat-header-text">
                  <h3 className="chat-title">–î–∂–∞—Ä–≤–∏—Å</h3>
                  <p className={`chat-status ${isSpeaking ? 'speaking' : ''}`}>
                    –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç ‚Ä¢ {isSpeaking ? '–ì–æ–≤–æ—Ä–∏—Ç –º–µ–¥–ª–µ–Ω–Ω–æ...' : '–û–Ω–ª–∞–π–Ω'}
                  </p>
                </div>
              </div>
              <button
                onClick={toggleChat}
                className="chat-close-button"
                aria-label="–ó–∞–∫—Ä—ã—Ç—å —á–∞—Ç"
              >
                <X className="chat-close-icon" />
              </button>
            </div>

            {/* –°–æ–æ–±—â–µ–Ω–∏—è */}
            <div className="chat-messages">
              {messages.map((message) => (
                <div
                  key={message.id}
                  className={`message ${message.sender === 'user' ? 'message-user' : 'message-jarvis'}`}
                >
                  <div className="message-avatar">
                    {message.sender === 'user' ? (
                      <User className="message-avatar-icon" />
                    ) : (
                      <Bot className="message-avatar-icon" />
                    )}
                  </div>
                  <div className="message-content">
                    <div className="message-text">{message.text}</div>
                    <div className="message-time">
                      {message.timestamp.toLocaleTimeString('ru-RU', { 
                        hour: '2-digit', 
                        minute: '2-digit' 
                      })}
                    </div>
                  </div>
                </div>
              ))}
              
              {isTyping && (
                <div className="message message-jarvis">
                  <div className="message-avatar">
                    <Bot className="message-avatar-icon" />
                  </div>
                  <div className="message-content">
                    <div className="typing-indicator">
                      <span></span>
                      <span></span>
                      <span></span>
                    </div>
                  </div>
                </div>
              )}
              <div ref={messagesEndRef} />
            </div>

            {/* –ü–æ–ª–µ –≤–≤–æ–¥–∞ */}
            <div className="chat-input-container">
              <div className="chat-input-wrapper">
                <input
                  ref={inputRef}
                  type="text"
                  value={inputMessage}
                  onChange={(e) => setInputMessage(e.target.value)}
                  onKeyPress={handleKeyPress}
                  onFocus={handleInputFocus}
                  onBlur={handleInputBlur}
                  placeholder={isRecording ? "–ì–æ–≤–æ—Ä–∏—Ç–µ..." : "–ù–∞–ø–∏—à–∏—Ç–µ —Å–æ–æ–±—â–µ–Ω–∏–µ..."}
                  className="chat-input"
                  disabled={isRecording}
                />
                {speechSupported && (
                  <button
                    onClick={toggleRecording}
                    className={`chat-mic-button ${isRecording ? 'recording' : ''}`}
                    aria-label={isRecording ? "–û—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–ø–∏—Å—å" : "–ù–∞—á–∞—Ç—å –≥–æ–ª–æ—Å–æ–≤—É—é –∑–∞–ø–∏—Å—å"}
                  >
                    {isRecording ? <MicOff className="chat-mic-icon" /> : <Mic className="chat-mic-icon" />}
                    {isListening && <div className="mic-pulse"></div>}
                  </button>
                )}
                <button
                  onClick={handleSendMessage}
                  disabled={!inputMessage.trim() || isRecording}
                  className="chat-send-button"
                  aria-label="–û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ"
                >
                  <Send className="chat-send-icon" />
                </button>
              </div>
            </div>
          </div>
        </div>
      )}
    </>
  )
}
